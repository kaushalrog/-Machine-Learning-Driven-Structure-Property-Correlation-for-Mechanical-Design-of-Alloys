# -*- coding: utf-8 -*-
"""NeuralNetwork1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10xuVxwgaygrByG2igxiz6-98mrROSsjC
"""

!pip install pandas numpy scikit-learn matplotlib seaborn openpyxl lazypredict

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

from lazypredict.Supervised import LazyClassifier

# === 1. Load Dataset ===
file_path = "/content/HEA_Phase_Dataset_v1d.xlsx"  # Adjust path if needed
df = pd.read_excel(file_path)

# === 2. Drop Non-Composition Columns ===
df.drop(columns=['Alloy_ID', 'Alloy', 'Sythesis_Route', 'References'], inplace=True, errors='ignore')

# === 3. Encode Categorical Columns (if any) ===
for col in df.select_dtypes(include=['object']).columns:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# === 4. Ensure All Columns Are Numeric ===
df = df.apply(pd.to_numeric, errors='coerce')

# === 5. Select Composition-Based Features ===
composition_features = ['Num_of_Elem', 'VEC', 'Atom.Size.Diff', 'Elect.Diff', 'dHmix', 'dSmix']
X = df[composition_features]
y = df.iloc[:, -1]  # Assuming the last column is the target (phase)

# === 6. Handle Missing Values ===
X.fillna(X.mean(), inplace=True)

# === 7. Standardize Features ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === 8. Train-Test Split ===
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# === 9. LazyPredict Benchmarking ===
print("\n=== LazyPredict Benchmarking ===")
lazy_clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = lazy_clf.fit(X_train, X_test, y_train, y_test)

# Show top models sorted by accuracy
print(models.sort_values("Accuracy", ascending=False))

# === 10. Initialize & Train MLP Classifier ===
print("\n=== MLP Classifier Training & Evaluation ===")
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# === 11. Make Predictions (Train & Test) ===
y_train_pred = mlp.predict(X_train)
y_test_pred = mlp.predict(X_test)

# === 12. Accuracy ===
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy : {test_acc:.4f}")

# === 13. Classification Reports ===
print("\n=== Classification Report - Training Data ===")
print(classification_report(y_train, y_train_pred))

print("\n=== Classification Report - Testing Data ===")
print(classification_report(y_test, y_test_pred))

# === 14. Confusion Matrix - Training Data ===
train_cm = confusion_matrix(y_train, y_train_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(train_cm, annot=True, fmt='d', cmap='Greens')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - MLP (Training Data)")
plt.tight_layout()
plt.show()

# === 15. Confusion Matrix - Testing Data ===
test_cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - MLP (Testing Data)")
plt.tight_layout()
plt.show()

# === 16. Training Loss Curve ===
plt.figure(figsize=(8, 5))
plt.plot(mlp.loss_curve_)
plt.title("MLP Training Loss Curve")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='Set2')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('PCA - Phase Distribution')
plt.colorbar(scatter, label='Phase Label')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.inspection import permutation_importance

# === Permutation Importance for MLP ===
result = permutation_importance(mlp, X_test, y_test, n_repeats=30, random_state=42, scoring='accuracy')

# Extract results
perm_importances = pd.Series(result.importances_mean, index=composition_features)

# === Plot Permutation Importances ===
plt.figure(figsize=(8, 6))
perm_importances.sort_values().plot(kind='barh')
plt.title("Permutation Feature Importance (MLP Classifier)")
plt.xlabel("Mean Importance")
plt.ylabel("Features")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.model_selection import cross_val_score
scores = cross_val_score(mlp, X_scaled, y, cv=5, scoring='f1_weighted')
print("Cross-validated F1 scores:", scores)
print("Mean F1 Score:", np.mean(scores))

from sklearn.metrics import balanced_accuracy_score

# === Train MLP Classifier (Already Done) ===
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

# === Predict with MLP ===
y_pred = mlp.predict(X_test)

# === Calculate Balanced Accuracy ===
balanced_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced Accuracy (MLP Classifier):", balanced_acc)

